 29%|███████████████████████████████████████████████▋                                                                                                                       | 10/35 [05:55<15:56, 38.27s/it] 31%|████████████████████████████████████████████████████▍                                                                                                                  | 11/35 [06:23<14:06, 35.25s/it] 34%|█████████████████████████████████████████████████████████▎                                                                                                             | 12/35 [07:02<13:55, 36.31s/it] 37%|██████████████████████████████████████████████████████████████                                                                                                         | 13/35 [07:27<11:59, 32.71s/it] 40%|██████████████████████████████████████████████████████████████████▊                                                                                                    | 14/35 [07:52<10:40, 30.52s/it] 43%|███████████████████████████████████████████████████████████████████████▌                                                                                               | 15/35 [08:22<10:09, 30.48s/it] 46%|████████████████████████████████████████████████████████████████████████████▎                                                                                          | 16/35 [08:52<09:36, 30.32s/it] 49%|█████████████████████████████████████████████████████████████████████████████████                                                                                      | 17/35 [09:32<09:58, 33.27s/it] 51%|█████████████████████████████████████████████████████████████████████████████████████▉                                                                                 | 18/35 [10:02<09:07, 32.19s/it] 54%|██████████████████████████████████████████████████████████████████████████████████████████▋                                                                            | 19/35 [10:41<09:05, 34.11s/it] 57%|███████████████████████████████████████████████████████████████████████████████████████████████▍                                                                       | 20/35 [11:38<10:15, 41.03s/it] 60%|████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                  | 21/35 [12:08<08:47, 37.70s/it] 63%|████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                              | 22/35 [12:30<07:11, 33.18s/it] 66%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                         | 23/35 [13:09<06:57, 34.80s/it] 69%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                    | 24/35 [13:53<06:54, 37.66s/it] 71%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                               | 25/35 [14:20<05:42, 34.25s/it] 74%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                           | 26/35 [14:48<04:53, 32.59s/it] 77%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                      | 27/35 [15:15<04:05, 30.69s/it] 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                 | 28/35 [15:57<03:59, 34.26s/it] 83%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                            | 29/35 [16:35<03:32, 35.45s/it] 86%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                       | 30/35 [17:07<02:51, 34.37s/it] 89%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                   | 31/35 [17:35<02:09, 32.43s/it] 91%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋              | 32/35 [18:20<01:48, 36.29s/it] 94%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍         | 33/35 [18:45<01:05, 32.82s/it] 97%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏    | 34/35 [19:11<00:30, 30.69s/it]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [19:35<00:00, 28.82s/it]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [19:35<00:00, 33.60s/it]
  0%|                                                                                                               | 0/5 [00:00<?, ?it/s] 20%|████████████████████▍                                                                                 | 1/5 [05:57<23:49, 357.48s/it] 40%|████████████████████████████████████████▊                                                             | 2/5 [10:34<16:40, 333.39s/it] 60%|█████████████████████████████████████████████████████████████▏                                        | 3/5 [14:50<10:20, 310.05s/it] 80%|█████████████████████████████████████████████████████████████████████████████████▌                    | 4/5 [19:03<04:53, 293.03s/it]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [23:56<00:00, 293.14s/it]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [23:56<00:00, 287.39s/it]
Train 8 clips
Test 35 clips
Model:  Conv3dModel(
  (cnn_layers): Sequential(
    (0): Conv3d(3, 32, kernel_size=(5, 5, 5), stride=(2, 2, 2))
    (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Dropout3d(p=0.2, inplace=False)
    (4): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2))
    (5): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU(inplace=True)
    (7): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Dropout3d(p=0.2, inplace=False)
  )
  (linear_layers): Sequential(
    (0): Linear(in_features=82560, out_features=256, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=256, out_features=128, bias=True)
    (3): ReLU(inplace=True)
    (4): Dropout3d(p=0.2, inplace=False)
    (5): Linear(in_features=128, out_features=4, bias=True)
  )
)
Next Batch ...  20  from  693
Next Batch ...  40  from  693
Next Batch ...  60  from  693
Next Batch ...  80  from  693
Next Batch ...  100  from  693
Next Batch ...  120  from  693
Next Batch ...  140  from  693
Next Batch ...  160  from  693
Next Batch ...  180  from  693
Next Batch ...  200  from  693
Next Batch ...  220  from  693
Next Batch ...  240  from  693
Next Batch ...  260  from  693
Next Batch ...  280  from  693
Next Batch ...  300  from  693
Next Batch ...  320  from  693
Next Batch ...  340  from  693
Next Batch ...  360  from  693
Next Batch ...  380  from  693
Next Batch ...  400  from  693
Next Batch ...  420  from  693
Next Batch ...  440  from  693
Next Batch ...  460  from  693
Next Batch ...  480  from  693
Next Batch ...  500  from  693
Next Batch ...  520  from  693
Next Batch ...  540  from  693
Next Batch ...  560  from  693
Next Batch ...  580  from  693
Next Batch ...  600  from  693
Next Batch ...  620  from  693
^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[ANext Batch ...  640  from  693
Next Batch ...  660  from  693
Next Batch ...  680  from  693
Next Batch ...  693  from  693
Traceback (most recent call last):
  File "main.py", line 113, in <module>
    epoch_test_loss, epoch_test_score = evaluate(model, device, optimizer, valid_loader)
  File "/data/cs230-road-accidents/evaluate.py", line 58, in evaluate
    clip_ids = np.concatenate(clip_ids).reshape(-1, 1)
NameError: name 'np' is not defined
  3%|█▎                                          | 1/35 [00:39<22:28, 39.66s/it]  6%|██▌                                         | 2/35 [01:18<21:39, 39.38s/it]  9%|███▊                                        | 3/35 [01:53<20:18, 38.07s/it] 11%|█████                                       | 4/35 [02:16<17:19, 33.52s/it] 14%|██████▎                                     | 5/35 [02:40<15:18, 30.62s/it] 17%|███████▌                                    | 6/35 [03:04<13:51, 28.66s/it] 20%|████████▊                                   | 7/35 [03:58<16:54, 36.24s/it] 23%|██████████                                  | 8/35 [04:39<17:00, 37.78s/it] 26%|███████████▎                                | 9/35 [05:05<14:51, 34.29s/it] 29%|████████████▎                              | 10/35 [05:51<15:41, 37.66s/it] 31%|█████████████▌                             | 11/35 [06:19<13:53, 34.73s/it] 34%|██████████████▋                            | 12/35 [06:57<13:45, 35.89s/it] 37%|███████████████▉                           | 13/35 [07:21<11:50, 32.31s/it] 40%|█████████████████▏                         | 14/35 [07:46<10:32, 30.12s/it] 43%|██████████████████▍                        | 15/35 [08:16<10:01, 30.08s/it] 46%|███████████████████▋                       | 16/35 [08:44<09:20, 29.47s/it] 49%|████████████████████▉                      | 17/35 [09:23<09:41, 32.33s/it] 54%|███████████████████████▎                   | 19/35 [10:30<08:53, 33.35s/it] 57%|████████████████████████▌                  | 20/35 [11:26<10:01, 40.13s/it] 60%|█████████████████████████▊                 | 21/35 [11:56<08:36, 36.89s/it] 63%|███████████████████████████                | 22/35 [12:18<07:02, 32.50s/it] 66%|████████████████████████████▎              | 23/35 [12:55<06:46, 33.90s/it] 69%|█████████████████████████████▍             | 24/35 [13:38<06:42, 36.63s/it] 71%|██████████████████████████████▋            | 25/35 [14:03<05:30, 33.06s/it] 74%|███████████████████████████████▉           | 26/35 [14:30<04:42, 31.36s/it] 77%|█████████████████████████████████▏         | 27/35 [14:55<03:55, 29.48s/it] 80%|██████████████████████████████████▍        | 28/35 [15:37<03:51, 33.13s/it] 83%|███████████████████████████████████▋       | 29/35 [16:14<03:26, 34.39s/it] 86%|████████████████████████████████████▊      | 30/35 [16:46<02:47, 33.48s/it] 89%|██████████████████████████████████████     | 31/35 [17:13<02:06, 31.63s/it] 91%|███████████████████████████████████████▎   | 32/35 [17:57<01:46, 35.49s/it] 94%|████████████████████████████████████████▌  | 33/35 [18:21<01:03, 31.94s/it] 97%|█████████████████████████████████████████▊ | 34/35 [18:47<00:30, 30.03s/it]100%|███████████████████████████████████████████| 35/35 [19:10<00:00, 28.19s/it]100%|███████████████████████████████████████████| 35/35 [19:10<00:00, 32.88s/it]
  0%|                                                     | 0/5 [00:00<?, ?it/s] 20%|████████▊                                   | 1/5 [05:46<23:06, 346.53s/it] 40%|█████████████████▌                          | 2/5 [10:16<16:11, 323.71s/it] 60%|██████████████████████████▍                 | 3/5 [14:28<10:03, 301.90s/it] 80%|███████████████████████████████████▏        | 4/5 [18:37<04:46, 286.23s/it]100%|████████████████████████████████████████████| 5/5 [23:32<00:00, 288.87s/it]100%|████████████████████████████████████████████| 5/5 [23:32<00:00, 282.54s/it]
Train 8 clips
Test 35 clips
Model:  Conv3dModel(
  (cnn_layers): Sequential(
    (0): Conv3d(3, 32, kernel_size=(5, 5, 5), stride=(2, 2, 2))
    (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Dropout3d(p=0.2, inplace=False)
    (4): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2))
    (5): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU(inplace=True)
    (7): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Dropout3d(p=0.2, inplace=False)
  )
  (linear_layers): Sequential(
    (0): Linear(in_features=82560, out_features=256, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=256, out_features=128, bias=True)
    (3): ReLU(inplace=True)
    (4): Dropout3d(p=0.2, inplace=False)
    (5): Linear(in_features=128, out_features=4, bias=True)
  )
)
^CTraceback (most recent call last):
  File "main.py", line 112, in <module>
    train_losses, train_scores = train_one_epoch(model, device, train_loader, optimizer, epoch)
  File "/data/cs230-road-accidents/train.py", line 38, in train_one_epoch
    for batch_idx, (clip_id, X, y, video_id) in enumerate(train_loader):
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 345, in __next__
    data = self._next_data()
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 385, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/data/cs230-road-accidents/datasets.py", line 27, in __getitem__
    video, audio, info, video_idx = self.video_clips.get_clip(idx)
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/torchvision/datasets/video_utils.py", line 308, in get_clip
    video, audio, info = read_video(video_path, start_pts, end_pts)
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/torchvision/io/video.py", line 264, in read_video
    vframes = [frame.to_rgb().to_ndarray() for frame in video_frames]
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/torchvision/io/video.py", line 264, in <listcomp>
    vframes = [frame.to_rgb().to_ndarray() for frame in video_frames]
KeyboardInterrupt
ERROR:libav.mov,mp4,m4a,3gp,3g2,mj2:moov atom not found
  3%|█▎                                          | 1/35 [00:38<21:59, 38.80s/it]  6%|██▌                                         | 2/35 [01:16<21:13, 38.58s/it]  9%|███▊                                        | 3/35 [01:52<20:02, 37.58s/it] 11%|█████                                       | 4/35 [02:15<17:10, 33.25s/it] 14%|██████▎                                     | 5/35 [02:39<15:15, 30.53s/it] 17%|███████▌                                    | 6/35 [03:04<13:57, 28.89s/it] 20%|████████▊                                   | 7/35 [04:03<17:41, 37.91s/it] 23%|██████████                                  | 8/35 [04:47<17:52, 39.74s/it] 26%|███████████▎                                | 9/35 [05:15<15:38, 36.10s/it] 29%|████████████▎                              | 10/35 [06:04<16:40, 40.02s/it] 31%|█████████████▌                             | 11/35 [06:34<14:48, 37.01s/it] 34%|██████████████▋                            | 12/35 [07:14<14:36, 38.13s/it] 37%|███████████████▉                           | 13/35 [07:39<12:31, 34.17s/it] 40%|█████████████████▏                         | 14/35 [08:06<11:10, 31.91s/it] 43%|██████████████████▍                        | 15/35 [08:38<10:35, 31.78s/it] 46%|███████████████████▋                       | 16/35 [09:09<09:59, 31.57s/it] 49%|████████████████████▉                      | 17/35 [09:52<10:30, 35.05s/it] 51%|██████████████████████                     | 18/35 [10:23<09:36, 33.94s/it] 54%|███████████████████████▎                   | 19/35 [11:03<09:32, 35.80s/it] 57%|████████████████████████▌                  | 20/35 [11:59<10:28, 41.87s/it] 60%|█████████████████████████▊                 | 21/35 [12:31<09:02, 38.72s/it] 63%|███████████████████████████                | 22/35 [12:53<07:19, 33.84s/it] 66%|████████████████████████████▎              | 23/35 [13:35<07:13, 36.11s/it] 69%|█████████████████████████████▍             | 24/35 [14:22<07:14, 39.49s/it] 71%|██████████████████████████████▋            | 25/35 [14:49<05:57, 35.75s/it] 74%|███████████████████████████████▉           | 26/35 [15:18<05:04, 33.86s/it] 77%|█████████████████████████████████▏         | 27/35 [15:45<04:14, 31.81s/it] 80%|██████████████████████████████████▍        | 28/35 [16:31<04:12, 36.02s/it] 83%|███████████████████████████████████▋       | 29/35 [17:12<03:44, 37.39s/it] 86%|████████████████████████████████████▊      | 30/35 [17:43<02:58, 35.63s/it] 89%|██████████████████████████████████████     | 31/35 [18:11<02:12, 33.17s/it] 91%|███████████████████████████████████████▎   | 32/35 [18:56<01:50, 36.75s/it] 94%|████████████████████████████████████████▌  | 33/35 [19:20<01:05, 32.99s/it] 97%|█████████████████████████████████████████▊ | 34/35 [19:46<00:30, 30.88s/it]100%|███████████████████████████████████████████| 35/35 [20:10<00:00, 28.77s/it]100%|███████████████████████████████████████████| 35/35 [20:10<00:00, 34.58s/it]
  0%|                                                     | 0/5 [00:00<?, ?it/s] 20%|████████▊                                   | 1/5 [05:53<23:32, 353.01s/it] 40%|█████████████████▌                          | 2/5 [10:24<16:25, 328.51s/it] 60%|██████████████████████████▍                 | 3/5 [14:35<10:10, 305.35s/it] 80%|███████████████████████████████████▏        | 4/5 [18:47<04:49, 289.35s/it]100%|████████████████████████████████████████████| 5/5 [23:41<00:00, 290.81s/it]100%|████████████████████████████████████████████| 5/5 [23:41<00:00, 284.38s/it]
Train 8 clips
Test 35 clips
Model:  Conv3dModel(
  (cnn_layers): Sequential(
    (0): Conv3d(3, 32, kernel_size=(5, 5, 5), stride=(2, 2, 2))
    (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Dropout3d(p=0.2, inplace=False)
    (4): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2))
    (5): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU(inplace=True)
    (7): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Dropout3d(p=0.2, inplace=False)
  )
  (linear_layers): Sequential(
    (0): Linear(in_features=82560, out_features=256, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=256, out_features=128, bias=True)
    (3): ReLU(inplace=True)
    (4): Dropout3d(p=0.2, inplace=False)
    (5): Linear(in_features=128, out_features=4, bias=True)
  )
)
Next Batch ...  20  from  693
Next Batch ...  40  from  693
Next Batch ...  60  from  693
Next Batch ...  80  from  693
Next Batch ...  100  from  693
Next Batch ...  120  from  693
Next Batch ...  140  from  693
Next Batch ...  160  from  693
Next Batch ...  180  from  693
Next Batch ...  200  from  693
Next Batch ...  220  from  693
Next Batch ...  240  from  693
Next Batch ...  260  from  693
Next Batch ...  280  from  693
Next Batch ...  300  from  693
Next Batch ...  320  from  693
Next Batch ...  340  from  693
Next Batch ...  360  from  693
Next Batch ...  380  from  693
Next Batch ...  400  from  693
Next Batch ...  420  from  693
Next Batch ...  440  from  693
Next Batch ...  460  from  693
Next Batch ...  480  from  693
Next Batch ...  500  from  693
Next Batch ...  520  from  693
Next Batch ...  540  from  693
Next Batch ...  560  from  693
Next Batch ...  580  from  693
Next Batch ...  600  from  693
Next Batch ...  620  from  693
Next Batch ...  640  from  693
Next Batch ...  660  from  693
Next Batch ...  680  from  693
Next Batch ...  693  from  693

Test set (693 samples): Average loss: 0.0633, Accuracy: 42.89%

Train losses:  [[1.42179358 1.30191052 1.16678345 1.15351737 1.01335895 1.01676905
  1.18529141 0.96069074]]
Train scores:  [[0.05       0.45       0.5        0.4        0.6        0.5
  0.35       0.46153846]]
Test losses:  [0.06333922]
Test scores:  [0.4289011]
Next Batch ...  20  from  693
Next Batch ...  40  from  693
Next Batch ...  60  from  693
Next Batch ...  80  from  693
Next Batch ...  100  from  693
Next Batch ...  120  from  693
Next Batch ...  140  from  693
Next Batch ...  160  from  693
Next Batch ...  180  from  693
Next Batch ...  200  from  693
Next Batch ...  220  from  693
Next Batch ...  240  from  693
Next Batch ...  260  from  693
Next Batch ...  280  from  693
Next Batch ...  300  from  693
Next Batch ...  320  from  693
Next Batch ...  340  from  693
Next Batch ...  360  from  693
Next Batch ...  380  from  693
Next Batch ...  400  from  693
Next Batch ...  420  from  693
Next Batch ...  440  from  693
Next Batch ...  460  from  693
Next Batch ...  480  from  693
Next Batch ...  500  from  693
Next Batch ...  520  from  693
Next Batch ...  540  from  693
Next Batch ...  560  from  693
Next Batch ...  580  from  693
Next Batch ...  600  from  693
Next Batch ...  620  from  693
Next Batch ...  640  from  693
Next Batch ...  660  from  693
Next Batch ...  680  from  693
Next Batch ...  693  from  693

Test set (693 samples): Average loss: 0.0609, Accuracy: 44.40%

Train losses:  [[1.42179358 1.30191052 1.16678345 1.15351737 1.01335895 1.01676905
  1.18529141 0.96069074]
 [0.86504376 0.88821524 0.80143279 1.06479895 0.93911457 0.83410281
  0.82419932 0.81352198]]
Train scores:  [[0.05       0.45       0.5        0.4        0.6        0.5
  0.35       0.46153846]
 [0.5        0.55       0.65       0.5        0.6        0.65
  0.65       0.69230769]]
Test losses:  [0.06333922 0.06087748]
Test scores:  [0.4289011  0.44395604]
Next Batch ...  20  from  693
Next Batch ...  40  from  693
Next Batch ...  60  from  693
Next Batch ...  80  from  693
Next Batch ...  100  from  693
Next Batch ...  120  from  693
Next Batch ...  140  from  693
Next Batch ...  160  from  693
Next Batch ...  180  from  693
Next Batch ...  200  from  693
Next Batch ...  220  from  693
Next Batch ...  240  from  693
Next Batch ...  260  from  693
Next Batch ...  280  from  693
Next Batch ...  300  from  693
Next Batch ...  320  from  693
Next Batch ...  340  from  693
Next Batch ...  360  from  693
Next Batch ...  380  from  693
Next Batch ...  400  from  693
Next Batch ...  420  from  693
Next Batch ...  440  from  693
Next Batch ...  460  from  693
Next Batch ...  480  from  693
Next Batch ...  500  from  693
Next Batch ...  520  from  693
Next Batch ...  540  from  693
Next Batch ...  560  from  693
Next Batch ...  580  from  693
Next Batch ...  600  from  693
Next Batch ...  620  from  693
Next Batch ...  640  from  693
Next Batch ...  660  from  693
Next Batch ...  680  from  693
Next Batch ...  693  from  693

Test set (693 samples): Average loss: 0.0614, Accuracy: 41.87%

Train losses:  [[1.42179358 1.30191052 1.16678345 1.15351737 1.01335895 1.01676905
  1.18529141 0.96069074]
 [0.86504376 0.88821524 0.80143279 1.06479895 0.93911457 0.83410281
  0.82419932 0.81352198]
 [0.87858468 0.77485383 0.87824965 0.68346447 0.84283888 0.63642859
  0.58604836 0.83567643]]
Train scores:  [[0.05       0.45       0.5        0.4        0.6        0.5
  0.35       0.46153846]
 [0.5        0.55       0.65       0.5        0.6        0.65
  0.65       0.69230769]
 [0.55       0.6        0.5        0.75       0.55       0.85
  0.75       0.46153846]]
Test losses:  [0.06333922 0.06087748 0.06143922]
Test scores:  [0.4289011  0.44395604 0.41868132]
Next Batch ...  20  from  693
Next Batch ...  40  from  693
Next Batch ...  60  from  693
Next Batch ...  80  from  693
Next Batch ...  100  from  693
Next Batch ...  120  from  693
Next Batch ...  140  from  693
Next Batch ...  160  from  693
Next Batch ...  180  from  693
Next Batch ...  200  from  693
Next Batch ...  220  from  693
Next Batch ...  240  from  693
Next Batch ...  260  from  693
Next Batch ...  280  from  693
Next Batch ...  300  from  693
Next Batch ...  320  from  693
Next Batch ...  340  from  693
Next Batch ...  360  from  693
Next Batch ...  380  from  693
Next Batch ...  400  from  693
Next Batch ...  420  from  693
Next Batch ...  440  from  693
Next Batch ...  460  from  693
Next Batch ...  480  from  693
Next Batch ...  500  from  693
Next Batch ...  520  from  693
Next Batch ...  540  from  693
Next Batch ...  560  from  693
Next Batch ...  580  from  693
Next Batch ...  600  from  693
Next Batch ...  620  from  693
Next Batch ...  640  from  693
Next Batch ...  660  from  693
Next Batch ...  680  from  693
Next Batch ...  693  from  693

Test set (693 samples): Average loss: 0.0621, Accuracy: 43.24%

Train losses:  [[1.42179358 1.30191052 1.16678345 1.15351737 1.01335895 1.01676905
  1.18529141 0.96069074]
 [0.86504376 0.88821524 0.80143279 1.06479895 0.93911457 0.83410281
  0.82419932 0.81352198]
 [0.87858468 0.77485383 0.87824965 0.68346447 0.84283888 0.63642859
  0.58604836 0.83567643]
 [0.73031569 0.68380159 0.60193866 0.73462385 0.65229863 0.61634874
  0.5351119  0.62577087]]
Train scores:  [[0.05       0.45       0.5        0.4        0.6        0.5
  0.35       0.46153846]
 [0.5        0.55       0.65       0.5        0.6        0.65
  0.65       0.69230769]
 [0.55       0.6        0.5        0.75       0.55       0.85
  0.75       0.46153846]
 [0.65       0.75       0.75       0.6        0.75       0.75
  0.9        0.76923077]]
Test losses:  [0.06333922 0.06087748 0.06143922 0.06205379]
Test scores:  [0.4289011  0.44395604 0.41868132 0.43241758]
Next Batch ...  20  from  693
Next Batch ...  40  from  693
Next Batch ...  60  from  693
Next Batch ...  80  from  693
Next Batch ...  100  from  693
Next Batch ...  120  from  693
Next Batch ...  140  from  693
Next Batch ...  160  from  693
Next Batch ...  180  from  693
Next Batch ...  200  from  693
Next Batch ...  220  from  693
Next Batch ...  240  from  693
Next Batch ...  260  from  693
Next Batch ...  280  from  693
Next Batch ...  300  from  693
Next Batch ...  320  from  693
Next Batch ...  340  from  693
Next Batch ...  360  from  693
Next Batch ...  380  from  693
Next Batch ...  400  from  693
Next Batch ...  420  from  693
Next Batch ...  440  from  693
Next Batch ...  460  from  693
Next Batch ...  480  from  693
Next Batch ...  500  from  693
Next Batch ...  520  from  693
Next Batch ...  540  from  693
Next Batch ...  560  from  693
Next Batch ...  580  from  693
Next Batch ...  600  from  693
Next Batch ...  620  from  693
Next Batch ...  640  from  693
Next Batch ...  660  from  693
Next Batch ...  680  from  693
Next Batch ...  693  from  693

Test set (693 samples): Average loss: 0.0634, Accuracy: 43.96%

Train losses:  [[1.42179358 1.30191052 1.16678345 1.15351737 1.01335895 1.01676905
  1.18529141 0.96069074]
 [0.86504376 0.88821524 0.80143279 1.06479895 0.93911457 0.83410281
  0.82419932 0.81352198]
 [0.87858468 0.77485383 0.87824965 0.68346447 0.84283888 0.63642859
  0.58604836 0.83567643]
 [0.73031569 0.68380159 0.60193866 0.73462385 0.65229863 0.61634874
  0.5351119  0.62577087]
 [0.56818497 0.49244443 0.60416752 0.57985175 0.6472525  0.87038755
  0.46764261 0.63021117]]
Train scores:  [[0.05       0.45       0.5        0.4        0.6        0.5
  0.35       0.46153846]
 [0.5        0.55       0.65       0.5        0.6        0.65
  0.65       0.69230769]
 [0.55       0.6        0.5        0.75       0.55       0.85
  0.75       0.46153846]
 [0.65       0.75       0.75       0.6        0.75       0.75
  0.9        0.76923077]
 [0.75       0.85       0.85       0.75       0.75       0.5
  0.8        0.61538462]]
Test losses:  [0.06333922 0.06087748 0.06143922 0.06205379 0.06343723]
Test scores:  [0.4289011  0.44395604 0.41868132 0.43241758 0.43956044]
Next Batch ...  20  from  693
Next Batch ...  40  from  693
Next Batch ...  60  from  693
Next Batch ...  80  from  693
Next Batch ...  100  from  693
Next Batch ...  120  from  693
Next Batch ...  140  from  693
Next Batch ...  160  from  693
Next Batch ...  180  from  693
Next Batch ...  200  from  693
Next Batch ...  220  from  693
Next Batch ...  240  from  693
Next Batch ...  260  from  693
Next Batch ...  280  from  693
Next Batch ...  300  from  693
Next Batch ...  320  from  693
Next Batch ...  340  from  693
Next Batch ...  360  from  693
Next Batch ...  380  from  693
Next Batch ...  400  from  693
Next Batch ...  420  from  693
Next Batch ...  440  from  693
Next Batch ...  460  from  693
Next Batch ...  480  from  693
Next Batch ...  500  from  693
Next Batch ...  520  from  693
Next Batch ...  540  from  693
Next Batch ...  560  from  693
Next Batch ...  580  from  693
Next Batch ...  600  from  693
Next Batch ...  620  from  693
Next Batch ...  640  from  693
Next Batch ...  660  from  693
Next Batch ...  680  from  693
Next Batch ...  693  from  693

Test set (693 samples): Average loss: 0.0649, Accuracy: 43.10%

Train losses:  [[1.42179358 1.30191052 1.16678345 1.15351737 1.01335895 1.01676905
  1.18529141 0.96069074]
 [0.86504376 0.88821524 0.80143279 1.06479895 0.93911457 0.83410281
  0.82419932 0.81352198]
 [0.87858468 0.77485383 0.87824965 0.68346447 0.84283888 0.63642859
  0.58604836 0.83567643]
 [0.73031569 0.68380159 0.60193866 0.73462385 0.65229863 0.61634874
  0.5351119  0.62577087]
 [0.56818497 0.49244443 0.60416752 0.57985175 0.6472525  0.87038755
  0.46764261 0.63021117]
 [0.37024483 0.52983737 0.55871308 0.56138194 0.44820985 0.37010452
  0.59261632 0.5447163 ]]
Train scores:  [[0.05       0.45       0.5        0.4        0.6        0.5
  0.35       0.46153846]
 [0.5        0.55       0.65       0.5        0.6        0.65
  0.65       0.69230769]
 [0.55       0.6        0.5        0.75       0.55       0.85
  0.75       0.46153846]
 [0.65       0.75       0.75       0.6        0.75       0.75
  0.9        0.76923077]
 [0.75       0.85       0.85       0.75       0.75       0.5
  0.8        0.61538462]
 [0.95       0.85       0.8        0.75       0.9        1.
  0.7        0.76923077]]
Test losses:  [0.06333922 0.06087748 0.06143922 0.06205379 0.06343723 0.0649094 ]
Test scores:  [0.4289011  0.44395604 0.41868132 0.43241758 0.43956044 0.43098901]
Next Batch ...  20  from  693
Next Batch ...  40  from  693
Next Batch ...  60  from  693
Next Batch ...  80  from  693
Next Batch ...  100  from  693
Next Batch ...  120  from  693
Next Batch ...  140  from  693
Next Batch ...  160  from  693
Next Batch ...  180  from  693
Next Batch ...  200  from  693
Next Batch ...  220  from  693
Next Batch ...  240  from  693
Next Batch ...  260  from  693
Next Batch ...  280  from  693
Next Batch ...  300  from  693
Next Batch ...  320  from  693
Next Batch ...  340  from  693
Next Batch ...  360  from  693
Next Batch ...  380  from  693
Next Batch ...  400  from  693
Next Batch ...  420  from  693
Next Batch ...  440  from  693
Next Batch ...  460  from  693
Next Batch ...  480  from  693
Next Batch ...  500  from  693
Next Batch ...  520  from  693
Next Batch ...  540  from  693
Next Batch ...  560  from  693
Next Batch ...  580  from  693
Next Batch ...  600  from  693
Next Batch ...  620  from  693
Next Batch ...  640  from  693
Next Batch ...  660  from  693
Next Batch ...  680  from  693
Next Batch ...  693  from  693

Test set (693 samples): Average loss: 0.0649, Accuracy: 47.47%

Train losses:  [[1.42179358 1.30191052 1.16678345 1.15351737 1.01335895 1.01676905
  1.18529141 0.96069074]
 [0.86504376 0.88821524 0.80143279 1.06479895 0.93911457 0.83410281
  0.82419932 0.81352198]
 [0.87858468 0.77485383 0.87824965 0.68346447 0.84283888 0.63642859
  0.58604836 0.83567643]
 [0.73031569 0.68380159 0.60193866 0.73462385 0.65229863 0.61634874
  0.5351119  0.62577087]
 [0.56818497 0.49244443 0.60416752 0.57985175 0.6472525  0.87038755
  0.46764261 0.63021117]
 [0.37024483 0.52983737 0.55871308 0.56138194 0.44820985 0.37010452
  0.59261632 0.5447163 ]
 [0.43593368 0.50468349 0.58038139 0.47194403 0.36812168 0.50615847
  0.42522413 0.37441066]]
Train scores:  [[0.05       0.45       0.5        0.4        0.6        0.5
  0.35       0.46153846]
 [0.5        0.55       0.65       0.5        0.6        0.65
  0.65       0.69230769]
 [0.55       0.6        0.5        0.75       0.55       0.85
  0.75       0.46153846]
 [0.65       0.75       0.75       0.6        0.75       0.75
  0.9        0.76923077]
 [0.75       0.85       0.85       0.75       0.75       0.5
  0.8        0.61538462]
 [0.95       0.85       0.8        0.75       0.9        1.
  0.7        0.76923077]
 [0.85       0.75       0.8        0.8        0.9        0.85
  0.85       0.92307692]]
Test losses:  [0.06333922 0.06087748 0.06143922 0.06205379 0.06343723 0.0649094
 0.06485861]
Test scores:  [0.4289011  0.44395604 0.41868132 0.43241758 0.43956044 0.43098901
 0.47472527]
Next Batch ...  20  from  693
Next Batch ...  40  from  693
Next Batch ...  60  from  693
Next Batch ...  80  from  693
Next Batch ...  100  from  693
Next Batch ...  120  from  693
Next Batch ...  140  from  693
Next Batch ...  160  from  693
Next Batch ...  180  from  693
Next Batch ...  200  from  693
Next Batch ...  220  from  693
Next Batch ...  240  from  693
Next Batch ...  260  from  693
Next Batch ...  280  from  693
Next Batch ...  300  from  693
Next Batch ...  320  from  693
Next Batch ...  340  from  693
Next Batch ...  360  from  693
Next Batch ...  380  from  693
Next Batch ...  400  from  693
Next Batch ...  420  from  693
Next Batch ...  440  from  693
Next Batch ...  460  from  693
Next Batch ...  480  from  693
Next Batch ...  500  from  693
Next Batch ...  520  from  693
Next Batch ...  540  from  693
Next Batch ...  560  from  693
Next Batch ...  580  from  693
Next Batch ...  600  from  693
Next Batch ...  620  from  693
Next Batch ...  640  from  693
Next Batch ...  660  from  693
Next Batch ...  680  from  693
Next Batch ...  693  from  693

Test set (693 samples): Average loss: 0.0665, Accuracy: 47.24%

Train losses:  [[1.42179358 1.30191052 1.16678345 1.15351737 1.01335895 1.01676905
  1.18529141 0.96069074]
 [0.86504376 0.88821524 0.80143279 1.06479895 0.93911457 0.83410281
  0.82419932 0.81352198]
 [0.87858468 0.77485383 0.87824965 0.68346447 0.84283888 0.63642859
  0.58604836 0.83567643]
 [0.73031569 0.68380159 0.60193866 0.73462385 0.65229863 0.61634874
  0.5351119  0.62577087]
 [0.56818497 0.49244443 0.60416752 0.57985175 0.6472525  0.87038755
  0.46764261 0.63021117]
 [0.37024483 0.52983737 0.55871308 0.56138194 0.44820985 0.37010452
  0.59261632 0.5447163 ]
 [0.43593368 0.50468349 0.58038139 0.47194403 0.36812168 0.50615847
  0.42522413 0.37441066]
 [0.34802109 0.47723073 0.36286053 0.52190185 0.27444267 0.37950224
  0.33149081 0.3115662 ]]
Train scores:  [[0.05       0.45       0.5        0.4        0.6        0.5
  0.35       0.46153846]
 [0.5        0.55       0.65       0.5        0.6        0.65
  0.65       0.69230769]
 [0.55       0.6        0.5        0.75       0.55       0.85
  0.75       0.46153846]
 [0.65       0.75       0.75       0.6        0.75       0.75
  0.9        0.76923077]
 [0.75       0.85       0.85       0.75       0.75       0.5
  0.8        0.61538462]
 [0.95       0.85       0.8        0.75       0.9        1.
  0.7        0.76923077]
 [0.85       0.75       0.8        0.8        0.9        0.85
  0.85       0.92307692]
 [1.         0.7        0.85       0.8        1.         1.
  1.         0.92307692]]
Test losses:  [0.06333922 0.06087748 0.06143922 0.06205379 0.06343723 0.0649094
 0.06485861 0.0665108 ]
Test scores:  [0.4289011  0.44395604 0.41868132 0.43241758 0.43956044 0.43098901
 0.47472527 0.47241758]
Next Batch ...  20  from  693
Next Batch ...  40  from  693
Next Batch ...  60  from  693
Next Batch ...  80  from  693
Next Batch ...  100  from  693
Next Batch ...  120  from  693
Next Batch ...  140  from  693
Next Batch ...  160  from  693
Next Batch ...  180  from  693
Next Batch ...  200  from  693
Next Batch ...  220  from  693
Next Batch ...  240  from  693
Next Batch ...  260  from  693
Next Batch ...  280  from  693
Next Batch ...  300  from  693
Next Batch ...  320  from  693
Next Batch ...  340  from  693
Next Batch ...  360  from  693
Next Batch ...  380  from  693
Next Batch ...  400  from  693
Next Batch ...  420  from  693
Next Batch ...  440  from  693
Next Batch ...  460  from  693
Next Batch ...  480  from  693
Next Batch ...  500  from  693
Next Batch ...  520  from  693
Next Batch ...  540  from  693
Next Batch ...  560  from  693
Next Batch ...  580  from  693
Next Batch ...  600  from  693
Next Batch ...  620  from  693
Next Batch ...  640  from  693
Next Batch ...  660  from  693
Next Batch ...  680  from  693
Next Batch ...  693  from  693

Test set (693 samples): Average loss: 0.0669, Accuracy: 47.54%

Train losses:  [[1.42179358 1.30191052 1.16678345 1.15351737 1.01335895 1.01676905
  1.18529141 0.96069074]
 [0.86504376 0.88821524 0.80143279 1.06479895 0.93911457 0.83410281
  0.82419932 0.81352198]
 [0.87858468 0.77485383 0.87824965 0.68346447 0.84283888 0.63642859
  0.58604836 0.83567643]
 [0.73031569 0.68380159 0.60193866 0.73462385 0.65229863 0.61634874
  0.5351119  0.62577087]
 [0.56818497 0.49244443 0.60416752 0.57985175 0.6472525  0.87038755
  0.46764261 0.63021117]
 [0.37024483 0.52983737 0.55871308 0.56138194 0.44820985 0.37010452
  0.59261632 0.5447163 ]
 [0.43593368 0.50468349 0.58038139 0.47194403 0.36812168 0.50615847
  0.42522413 0.37441066]
 [0.34802109 0.47723073 0.36286053 0.52190185 0.27444267 0.37950224
  0.33149081 0.3115662 ]
 [0.35140324 0.25480872 0.43385005 0.44858035 0.43312684 0.23110604
  0.27496043 0.34332058]]
Train scores:  [[0.05       0.45       0.5        0.4        0.6        0.5
  0.35       0.46153846]
 [0.5        0.55       0.65       0.5        0.6        0.65
  0.65       0.69230769]
 [0.55       0.6        0.5        0.75       0.55       0.85
  0.75       0.46153846]
 [0.65       0.75       0.75       0.6        0.75       0.75
  0.9        0.76923077]
 [0.75       0.85       0.85       0.75       0.75       0.5
  0.8        0.61538462]
 [0.95       0.85       0.8        0.75       0.9        1.
  0.7        0.76923077]
 [0.85       0.75       0.8        0.8        0.9        0.85
  0.85       0.92307692]
 [1.         0.7        0.85       0.8        1.         1.
  1.         0.92307692]
 [0.9        1.         0.9        0.9        0.8        0.95
  1.         0.76923077]]
Test losses:  [0.06333922 0.06087748 0.06143922 0.06205379 0.06343723 0.0649094
 0.06485861 0.0665108  0.06691623]
Test scores:  [0.4289011  0.44395604 0.41868132 0.43241758 0.43956044 0.43098901
 0.47472527 0.47241758 0.47538462]
Next Batch ...  20  from  693
Next Batch ...  40  from  693
Next Batch ...  60  from  693
Next Batch ...  80  from  693
Next Batch ...  100  from  693
Next Batch ...  120  from  693
Next Batch ...  140  from  693
Next Batch ...  160  from  693
Next Batch ...  180  from  693
Next Batch ...  200  from  693
Next Batch ...  220  from  693
Next Batch ...  240  from  693
Next Batch ...  260  from  693
Next Batch ...  280  from  693
Next Batch ...  300  from  693
Next Batch ...  320  from  693
Next Batch ...  340  from  693
Next Batch ...  360  from  693
Next Batch ...  380  from  693
Next Batch ...  400  from  693
Next Batch ...  420  from  693
Next Batch ...  440  from  693
Next Batch ...  460  from  693
Next Batch ...  480  from  693
Next Batch ...  500  from  693
Next Batch ...  520  from  693
Next Batch ...  540  from  693
Next Batch ...  560  from  693
Next Batch ...  580  from  693
Next Batch ...  600  from  693
Next Batch ...  620  from  693
Next Batch ...  640  from  693
Next Batch ...  660  from  693
Next Batch ...  680  from  693
Next Batch ...  693  from  693

Test set (693 samples): Average loss: 0.0677, Accuracy: 47.74%

Train losses:  [[1.42179358 1.30191052 1.16678345 1.15351737 1.01335895 1.01676905
  1.18529141 0.96069074]
 [0.86504376 0.88821524 0.80143279 1.06479895 0.93911457 0.83410281
  0.82419932 0.81352198]
 [0.87858468 0.77485383 0.87824965 0.68346447 0.84283888 0.63642859
  0.58604836 0.83567643]
 [0.73031569 0.68380159 0.60193866 0.73462385 0.65229863 0.61634874
  0.5351119  0.62577087]
 [0.56818497 0.49244443 0.60416752 0.57985175 0.6472525  0.87038755
  0.46764261 0.63021117]
 [0.37024483 0.52983737 0.55871308 0.56138194 0.44820985 0.37010452
  0.59261632 0.5447163 ]
 [0.43593368 0.50468349 0.58038139 0.47194403 0.36812168 0.50615847
  0.42522413 0.37441066]
 [0.34802109 0.47723073 0.36286053 0.52190185 0.27444267 0.37950224
  0.33149081 0.3115662 ]
 [0.35140324 0.25480872 0.43385005 0.44858035 0.43312684 0.23110604
  0.27496043 0.34332058]
 [0.23963031 0.27200291 0.23017772 0.32437006 0.2303222  0.26353019
  0.33695459 0.48129866]]
Train scores:  [[0.05       0.45       0.5        0.4        0.6        0.5
  0.35       0.46153846]
 [0.5        0.55       0.65       0.5        0.6        0.65
  0.65       0.69230769]
 [0.55       0.6        0.5        0.75       0.55       0.85
  0.75       0.46153846]
 [0.65       0.75       0.75       0.6        0.75       0.75
  0.9        0.76923077]
 [0.75       0.85       0.85       0.75       0.75       0.5
  0.8        0.61538462]
 [0.95       0.85       0.8        0.75       0.9        1.
  0.7        0.76923077]
 [0.85       0.75       0.8        0.8        0.9        0.85
  0.85       0.92307692]
 [1.         0.7        0.85       0.8        1.         1.
  1.         0.92307692]
 [0.9        1.         0.9        0.9        0.8        0.95
  1.         0.76923077]
 [0.95       1.         1.         0.9        1.         0.95
  0.9        0.76923077]]
Test losses:  [0.06333922 0.06087748 0.06143922 0.06205379 0.06343723 0.0649094
 0.06485861 0.0665108  0.06691623 0.06771067]
Test scores:  [0.4289011  0.44395604 0.41868132 0.43241758 0.43956044 0.43098901
 0.47472527 0.47241758 0.47538462 0.47736264]
